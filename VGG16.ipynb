{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''\n",
    "VGG16 Network trained on CIFAR-10\n",
    "\n",
    "Network architecture:\n",
    "\n",
    "Input - 224 x 224\n",
    "Conv1 - 3 x 3 x 64 - ReLU\n",
    "Conv2 - 3 x 3 x 64 - ReLU - Maxpool\n",
    "Conv3 - 3 x 3 x 128 - ReLU\n",
    "Conv4 - 3 x 3 x 128 - ReLU - Maxpool\n",
    "Conv5 - 3 x 3 x 256 - ReLU\n",
    "Conv6 - 3 x 3 x 256 - ReLU\n",
    "Conv7 - 1 x 1 x 256 - ReLU - Maxpool\n",
    "Conv9 - 3 x 3 x 512 - ReLU\n",
    "Conv10 - 3 x 3 x 512 - ReLU\n",
    "Conv11 - 1 x 1 x 512 - ReLU - Maxpool\n",
    "Conv12 - 3 x 3 x 512 - ReLU\n",
    "Conv13 - 3 x 3 x 512 - ReLU\n",
    "Conv14 - 1 x 1 x 512 - ReLU - Maxpool\n",
    "FC1 - 4096 - ReLU\n",
    "FC2 - 4096 - ReLU\n",
    "FC3 - 1000 - Softmax\n",
    "\n",
    "*All conv have stride 1\n",
    "*All max-pool have stride 2 and filter size 2 x 2\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import cPickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#weigths_file = '/home/salman/tensorflow/KittiSeg/DATA/vgg16.npy'\n",
    "#weights = np.load(weigths_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Shape: \n",
      "(50000, 3072)\n",
      "Test Data Shape: \n",
      "(10000, 3072)\n",
      "Train Labels Shape: \n",
      "(50000, 10)\n",
      "Test Labels Shape: \n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# Import the dataset\n",
    "data_path = '/media/salman/DATA/General Datasets/cifar-10-batches-py/'\n",
    "\n",
    "# Specify the list of files in the data path\n",
    "train_batches = ['data_batch_1', 'data_batch_2', 'data_batch_3', 'data_batch_4', 'data_batch_5']\n",
    "test_batch = 'test_batch'\n",
    "\n",
    "# Load the training data\n",
    "tr_data = np.ndarray([50000, 3072])\n",
    "tr_labels = np.ndarray([50000], dtype=int)\n",
    "\n",
    "for i in range(len(train_batches)):\n",
    "    f = train_batches[i]\n",
    "    path = data_path + f\n",
    "    with open(path, 'rb') as fo:\n",
    "        dict = cPickle.load(fo)\n",
    "        data = dict['data']\n",
    "        labels = dict['labels']\n",
    "        tr_data[i*10000:(i+1)*10000,:] = data\n",
    "        tr_labels[i*10000:(i+1)*10000] = labels\n",
    "\n",
    "print 'Train Data Shape: '\n",
    "print tr_data.shape\n",
    "\n",
    "# Load the test data\n",
    "path = data_path + test_batch\n",
    "with open(path, 'rb') as fo:\n",
    "    dict = cPickle.load(fo)\n",
    "    te_data = dict['data']\n",
    "    te_labels = dict['labels']\n",
    "    \n",
    "print 'Test Data Shape: '\n",
    "print te_data.shape\n",
    "\n",
    "# Convert labels to one hot encoding\n",
    "train_labels = np.zeros([50000, 10], dtype=int)\n",
    "train_labels[np.arange(50000), tr_labels] = 1\n",
    "print 'Train Labels Shape: '\n",
    "print train_labels.shape\n",
    "\n",
    "test_labels = np.zeros([10000, 10], dtype = int)\n",
    "test_labels[np.arange(10000), te_labels] = 1\n",
    "print 'Test Labels Shape: '\n",
    "print test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data New Shape: \n",
      "(50000, 32, 32, 3)\n",
      "Test Data New Shape: \n",
      "(10000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "# Reshape training and test images from vectors to matrices\n",
    "train_data = np.reshape(tr_data, newshape=[50000, 32, 32, 3])\n",
    "print 'Train Data New Shape: '\n",
    "print train_data.shape\n",
    "test_data = np.reshape(te_data, newshape=[10000, 32, 32, 3])\n",
    "print 'Test Data New Shape: '\n",
    "print test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 32, 32, 3]) # Input image\n",
    "y = tf.placeholder(tf.float32, [None, num_classes]) # Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer 1\n",
    "W_conv1 = tf.Variable(tf.truncated_normal([3, 3, 3, 64], stddev= 0.1))\n",
    "b_conv1 = tf.Variable(tf.constant(0.1, shape=[64]))\n",
    "\n",
    "convolve1 = tf.nn.conv2d(X, W_conv1, strides=[1,1,1,1], padding='VALID') + b_conv1\n",
    "conv1 = tf.nn.relu(convolve1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer 2\n",
    "W_conv2 = tf.Variable(tf.truncated_normal([3, 3, 64, 64], stddev= 0.1))\n",
    "b_conv2 = tf.Variable(tf.constant(0.1, shape=[64]))\n",
    "\n",
    "convolve2 = tf.nn.conv2d(conv1, W_conv2, strides=[1,1,1,1], padding='VALID') + b_conv2\n",
    "h_conv2 = tf.nn.relu(convolve2)\n",
    "conv2 = tf.nn.max_pool(h_conv2, ksize=[1,2,2,1],strides=[1,2,2,1], padding='VALID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer 3\n",
    "W_conv3 = tf.Variable(tf.truncated_normal([3, 3, 64, 128], stddev= 0.1))\n",
    "b_conv3 = tf.Variable(tf.constant(0.1, shape=[128]))\n",
    "\n",
    "convolve3 = tf.nn.conv2d(conv2, W_conv3, strides=[1,1,1,1], padding='VALID') + b_conv3\n",
    "conv3 = tf.nn.relu(convolve3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer 4\n",
    "W_conv4 = tf.Variable(tf.truncated_normal([3, 3, 128, 128], stddev= 0.1))\n",
    "b_conv4 = tf.Variable(tf.constant(0.1, shape=[128]))\n",
    "\n",
    "convolve4 = tf.nn.conv2d(conv3, W_conv4, strides=[1,1,1,1], padding='VALID') + b_conv4\n",
    "h_conv4 = tf.nn.relu(convolve4)\n",
    "conv4 = tf.nn.max_pool(h_conv4, ksize=[1,2,2,1],strides=[1,2,2,1], padding='VALID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer 5\n",
    "W_conv5 = tf.Variable(tf.truncated_normal([3, 3, 128, 256], stddev= 0.1))\n",
    "b_conv5 = tf.Variable(tf.constant(0.1, shape=[256]))\n",
    "\n",
    "convolve5 = tf.nn.conv2d(conv4, W_conv5, strides=[1,1,1,1], padding='VALID') + b_conv5\n",
    "conv5 = tf.nn.relu(convolve5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer 6\n",
    "W_conv6 = tf.Variable(tf.truncated_normal([3, 3, 256, 256], stddev= 0.1))\n",
    "b_conv6 = tf.Variable(tf.constant(0.1, shape=[256]))\n",
    "\n",
    "convolve6 = tf.nn.conv2d(conv5, W_conv6, strides=[1,1,1,1], padding='VALID') + b_conv6\n",
    "conv6 = tf.nn.relu(convolve6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer 7\n",
    "W_conv7 = tf.Variable(tf.truncated_normal([1, 1, 256, 256], stddev= 0.1))\n",
    "b_conv7 = tf.Variable(tf.constant(0.1, shape=[256]))\n",
    "\n",
    "convolve7 = tf.nn.conv2d(conv6, W_conv7, strides=[1,1,1,1], padding='VALID') + b_conv7\n",
    "h_conv7 = tf.nn.relu(convolve7)\n",
    "conv7 = tf.nn.max_pool(h_conv7, ksize=[1,2,2,1],strides=[1,2,2,1], padding='VALID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer 8\n",
    "W_conv8 = tf.Variable(tf.truncated_normal([3, 3, 256, 512], stddev= 0.1))\n",
    "b_conv8 = tf.Variable(tf.constant(0.1, shape=[512]))\n",
    "\n",
    "convolve8 = tf.nn.conv2d(conv7, W_conv8, strides=[1,1,1,1], padding='VALID') + b_conv8\n",
    "conv8 = tf.nn.relu(convolve8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer 9\n",
    "W_conv9 = tf.Variable(tf.truncated_normal([3, 3, 512, 512], stddev= 0.1))\n",
    "b_conv9 = tf.Variable(tf.constant(0.1, shape=[512]))\n",
    "\n",
    "convolve9 = tf.nn.conv2d(conv8, W_conv9, strides=[1,1,1,1], padding='VALID') + b_conv9\n",
    "conv9 = tf.nn.relu(convolve9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer 10\n",
    "W_conv10 = tf.Variable(tf.truncated_normal([1, 1, 512, 512], stddev= 0.1))\n",
    "b_conv10 = tf.Variable(tf.constant(0.1, shape=[512]))\n",
    "\n",
    "convolve10 = tf.nn.conv2d(conv9, W_conv10, strides=[1,1,1,1], padding='VALID') + b_conv10\n",
    "h_conv10 = tf.nn.relu(convolve10)\n",
    "conv10 = tf.nn.max_pool(h_conv10, ksize=[1,2,2,1],strides=[1,2,2,1], padding='VALID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer 11\n",
    "W_conv11 = tf.Variable(tf.truncated_normal([3, 3, 512, 512], stddev= 0.1))\n",
    "b_conv11 = tf.Variable(tf.constant(0.1, shape=[512]))\n",
    "\n",
    "convolve11 = tf.nn.conv2d(conv10, W_conv11, strides=[1,1,1,1], padding='VALID') + b_conv11\n",
    "conv11 = tf.nn.relu(convolve11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer 12\n",
    "W_conv12 = tf.Variable(tf.truncated_normal([3, 3, 512, 512], stddev= 0.1))\n",
    "b_conv12 = tf.Variable(tf.constant(0.1, shape=[512]))\n",
    "\n",
    "convolve12 = tf.nn.conv2d(conv11, W_conv12, strides=[1,1,1,1], padding='VALID') + b_conv12\n",
    "conv12 = tf.nn.relu(convolve12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer 13\n",
    "W_conv13 = tf.Variable(tf.truncated_normal([1, 1, 512, 512], stddev= 0.1))\n",
    "b_conv13 = tf.Variable(tf.constant(0.1, shape=[512]))\n",
    "\n",
    "convolve13 = tf.nn.conv2d(conv12, W_conv13, strides=[1,1,1,1], padding='VALID') + b_conv13\n",
    "h_conv13 = tf.nn.relu(convolve13)\n",
    "conv13 = tf.nn.max_pool(h_conv13, ksize=[1,2,2,1],strides=[1,2,2,1], padding='VALID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer 14 (FC)\n",
    "layer13_flat = tf.reshape(conv13, [-1, 3*3*512])\n",
    "\n",
    "W_fc1 = tf.Variable(tf.truncated_normal([3*3*512, 4096], stddev= 0.1))\n",
    "b_fc1 = tf.Variable(tf.constant(0.1, shape=[4096]))\n",
    "\n",
    "_fc1 = tf.add(tf.matmul(layer13_flat, W_fc1), b_fc1)\n",
    "fc1 = tf.nn.relu(_fc1)\n",
    "\n",
    "# Dropout 14\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "fc1_drop = tf.nn.dropout(fc1, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer 15 (FC)\n",
    "W_fc2 = tf.Variable(tf.truncated_normal([4096, 4096], stddev= 0.1))\n",
    "b_fc2 = tf.Variable(tf.constant(0.1, shape=[4096]))\n",
    "\n",
    "_fc2 = tf.add(tf.matmul(fc1_drop, W_fc2), b_fc2)\n",
    "fc2 = tf.nn.relu(_fc2)\n",
    "\n",
    "# Dropout 14\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "fc2_drop = tf.nn.dropout(fc2, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Softmax_1:0' shape=(?, 1000) dtype=float32>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Layer 16 (FC)\n",
    "W_fc3 = tf.Variable(tf.truncated_normal([4096, num_classes], stddev= 0.1))\n",
    "b_fc3 = tf.Variable(tf.constant(0.1, shape=[num_classes]))\n",
    "\n",
    "_fc3 = tf.add(tf.matmul(fc2_drop, W_fc3), b_fc3)\n",
    "fc3 = tf.nn.relu(_fc3)\n",
    "\n",
    "# Softmax layer\n",
    "y_pred = tf.nn.softmax(fc3)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "cross_entroy = tf.reduce_mean(-tf.reduce_sum(y *tf.log(y_pred), reduction_indices=[1]))\n",
    "\n",
    "# Optimizer\n",
    "learning_rate = tf.placeholder(tf.float32)\n",
    "train_step = tf.train.AdamOptimizer(learning_rate).minimize(cross_entroy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
